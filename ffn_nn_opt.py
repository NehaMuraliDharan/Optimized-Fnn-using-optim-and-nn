# -*- coding: utf-8 -*-
"""FFN_NN_OPT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xu7Rd7xyXVq8nfqS7dfD3cFM15BZqnqb
"""

import numpy as np
import math 
import matplotlib.pyplot as plt
import matplotlib.colors
import pandas as pd 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import accuracy_score, mean_squared_error, log_loss
from tqdm import tqdm_notebook 
import seaborn as sns
import time 
from  IPython.display import HTML
import warnings 
warnings. filterwarnings('ignore')

from sklearn.preprocessing import OneHotEncoder 
from sklearn.datasets import make_blobs 

import torch

torch.manual_seed(0)
my_cmap=matplotlib.colors.LinearSegmentedColormap.from_list('', ['violet','black','darkblue'])
data, labels= make_blobs(n_samples=1000, n_features=2, centers=4, random_state=0)
print(data.shape, labels.shape)
plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=my_cmap)
plt.show()

X_train, X_test, Y_train, Y_test= train_test_split(data, labels, stratify=labels, random_state=0)
X_train, X_test, Y_train, Y_test= map(torch.tensor, (X_train, X_test, Y_train, Y_test))
print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)

def model(x):
  a1 = torch.matmul(x, weights1) + bias1 # (N, 2) x (2, 2) -> (N, 2)
  h1 = a1.sigmoid() # (N, 2)
  a2 = torch.matmul(h1, weights2) + bias2 # (N, 2) x (2, 4) -> (N, 4)
  h2 = a2.exp()/a2.exp().sum(-1).unsqueeze(-1) # (N, 4)
  return h2

def loss_fn(y_hat, y):
  return -(y_hat[range(y.shape[0]), y].log()).mean()

def accuracy(y_hat, y):
  pred = torch.argmax(y_hat, dim=1)
  return (pred == y).float().mean()

import torch.nn.functional as F

torch.manual_seed(0)
weights1 = torch.randn(2, 2) / math.sqrt(2)
weights1.requires_grad_()
bias1 = torch.zeros(2, requires_grad=True)

weights2 = torch.randn(2, 4) / math.sqrt(2)
weights2.requires_grad_()
bias2 = torch.zeros(4, requires_grad=True)

learning_rate = 0.2
epochs = 10000

X_train = X_train.float()
Y_train = Y_train.long()

loss_arr = []
acc_arr = []

for epoch in range(epochs):
  y_hat = model(X_train)
  loss = F.cross_entropy(y_hat, Y_train)
  loss.backward()
  loss_arr.append(loss.item())
  acc_arr.append(accuracy(y_hat, Y_train))

  with torch.no_grad():
    weights1 -= weights1.grad * learning_rate
    bias1 -= bias1.grad * learning_rate
    weights2 -= weights2.grad * learning_rate
    bias2 -= bias2.grad * learning_rate
    weights1.grad.zero_()
    bias1.grad.zero_()
    weights2.grad.zero_()
    bias2.grad.zero_()

plt.plot(loss_arr, 'r-')
plt.plot(acc_arr, 'b-')
plt.show()
print('Loss before training', loss_arr[0])
print('Loss after training', loss_arr[-1])

#NN PARAMETER 
import torch.nn as nn

class FirstNetwork(nn.Module):
  def __init__(self):
    super().__init__()
    torch.manual_seed(0)
    self.weights1=nn.Parameter(torch.randn(2,2)/math.sqrt(2))
    self.bias1=nn.Parameter(torch.zeros(2))
    self.weights2=nn.Parameter(torch.randn(2,4)/math.sqrt(2))
    self.bias2=nn.Parameter(torch.zeros(4))
  
  def forward(self, X):
    a1=torch.matmul(X, self.weights1)+self.bias1
    h1=a1.sigmoid()
    a2=torch.matmul(h1, self.weights2)+self.bias2
    h2=a2.exp()/a2.exp().sum(-1).unsqueeze(-1)
    return h2

def fit(epochs =10000, learning_rate=1):
  loss_arr=[]
  acc_arr=[]
  for epoch in range(epochs):
    y_hat=fn(X_train)
    loss=F.cross_entropy(y_hat, Y_train)
    loss_arr.append(loss.item())
    acc_arr.append(accuracy(y_hat, Y_train))

    loss.backward()
    with torch.no_grad():
      for param in fn.parameters():
        param -= param.grad*learning_rate
      fn.zero_grad()
  plt.plot(loss_arr, 'r-')
  plt.plot(acc_arr, 'b-')
  plt.show()
  print('Loss before training', loss_arr[0])
  print('Loss after training', loss_arr[-1])

fn=FirstNetwork()
fit()

class FirstNetwork_v1(nn.Module):
  
  def __init__(self):
    super().__init__()
    torch.manual_seed(0)
    self.lin1 = nn.Linear(2, 2)
    self.lin2 = nn.Linear(2, 4)
    
  def forward(self, X):
    a1 = self.lin1(X)
    h1 = a1.sigmoid()
    a2 = self.lin2(h1)
    h2 = a2.exp()/a2.exp().sum(-1).unsqueeze(-1)
    return h2

from torch import optim
def fit_v1(epochs = 1000, learning_rate = 1):
  loss_arr = []
  acc_arr = []
  opt = optim.SGD(fn.parameters(), lr=learning_rate)
  
  for epoch in range(epochs):
    y_hat = fn(X_train)
    loss = F.cross_entropy(y_hat, Y_train)
    loss_arr.append(loss.item())
    acc_arr.append(accuracy(y_hat, Y_train))

    loss.backward()
    opt.step()
    opt.zero_grad()
        
  plt.plot(loss_arr, 'r-')
  plt.plot(acc_arr, 'b-')
  plt.show()      
  print('Loss before training', loss_arr[0])
  print('Loss after training', loss_arr[-1])

fn = FirstNetwork_v1()
fit_v1()

